# -*- coding: utf-8 -*-
"""LLMs_RAG.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1bcCAiFD-vPq2x0n5mDOzQ8mrsKmsaO9l
"""

#!pip install -q transformers==4.41.2
#!pip install -q bitsandbytes==0.43.1
#!pip install -q accelerate==0.31.0
#!pip install -q langchain==0.2.5
#!pip install -q langchainhub==0.1.20
#!pip install -q langchain-chroma==0.1.1
#!pip install -q langchain-community==0.2.5
#!pip install -q langchain_huggingface==0.0.3
#!pip install -q python-dotenv==1.0.1
#!pip install -q pypdf==4.2.0
#!pip install -q numpy==1.24.4

#!pip install numpy==1.25

import torch #type: ignore 

from transformers import BitsAndBytesConfig #type: ignore
from transformers import AutoTokenizer , AutoModelForCausalLM , pipeline #type: ignore
from langchain_huggingface import HuggingFaceEmbeddings #type: ignore
from langchain_huggingface . llms import HuggingFacePipeline #type: ignore

from langchain . memory import ConversationBufferMemory #type: ignore
from langchain_community . chat_message_histories import ChatMessageHistory #type: ignore
from langchain_community . document_loaders import PyPDFLoader , TextLoader #type: ignore
from langchain . chains import ConversationalRetrievalChain #type: ignore

from langchain_chroma import Chroma #type: ignore
from langchain_text_splitters import RecursiveCharacterTextSplitter #type: ignore
from langchain_core . runnables import RunnablePassthrough #type: ignore
from langchain_core . output_parsers import StrOutputParser #type: ignore
from langchain import hub #type: ignore

Loader = PyPDFLoader
FILE_PATH = '/content/YOLOv10_Tutorials.pdf'
loader = Loader(FILE_PATH)
documents = loader.load()

text_splitter = RecursiveCharacterTextSplitter( chunk_size =1000 ,chunk_overlap =100)

docs = text_splitter.split_documents(documents)

print( " Number of sub - documents : ", len(docs) )
print(docs[0])

embedding = HuggingFaceEmbeddings()

vector_db = Chroma.from_documents( documents =docs ,embedding = embedding )

retriever = vector_db.as_retriever()
#Ta có thể thử thực hiện truy vấn với một đoạn văn bản bất kì tại đây:
result = retriever.invoke(" What is YOLO ?")

print(" Number of relevant documents : ", len ( result ) )

nf4_config = BitsAndBytesConfig (load_in_4bit =True ,
bnb_4bit_quant_type =" nf4 ",
bnb_4bit_use_double_quant =True ,
bnb_4bit_compute_dtype = torch.bfloat16
)

MODEL_NAME = "lmsys/vicuna-7b-v1.5"

nf4_config = BitsAndBytesConfig (
    load_in_4bit =True ,
    bnb_4bit_quant_type ="nf4", # Use nf4 quantization
    bnb_4bit_use_double_quant =True ,
    bnb_4bit_compute_dtype = torch.bfloat16
)

model = AutoModelForCausalLM.from_pretrained(
    MODEL_NAME,
    quantization_config = nf4_config,
    low_cpu_mem_usage = True
)

tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME) # Use the same model nam

model_pipeline = pipeline(
    "text-generation",  # Remove the space between 'text' and '-'
    model=model,
    tokenizer=tokenizer,
    max_new_tokens=512,
    pad_token_id=tokenizer.eos_token_id,
    device_map="auto"
)

llm = HuggingFacePipeline(
 pipeline = model_pipeline ,
)

prompt = hub.pull("rlm/rag-prompt")

def format_docs(docs):
    return "\n\n".join(doc.page_content for doc in docs)

rag_chain = (
    {"context": retriever | format_docs, "question": RunnablePassthrough()}
    | prompt
    | llm
    | StrOutputParser ()
)

USER_QUESTION = "YOLOv10 là gì?"
output = rag_chain.invoke(USER_QUESTION) # Pass the question in a dictionary with the correct key
# Check if 'Answer :' is in the output before splitting
if 'Answer :' in output:
    answer = output.split('Answer :')[1].strip()
    print(answer)
else:
    print("The output does not contain an answer.")
    print(output) # Print the raw output for debugging



